亚龙大模型概述
===
# 现有模型功能
- 多功能对话
- 亚龙大模型进行原生问答
- 通过MCP调用其他工具辅助问答。该功能由于生成效果不稳定因此暂不开放。已完成功能：
    - [x] 互联网查询
    - [x] 数学计算器
    - 后期可以通过注册Agent定制更多能力来增强模型的功能。
- 会话管理
    - 通过导出会话为markdown文件以保存记录，暂不支持导入对话
    - 新建会话以实现不同场景的问答

### **RAG对话**
- 知识库对话
    - 通过查询本地知识库来进行辅助回答。可以通过调整top_k参数以及知识分数分配阈值来控制查询结果。
- 文件对话
    - 通过将上传的文件向量化并缓存以实现类似知识库问答的效果。

# 研发过程中存在的问题
### 常规模型对大量资料难以细致处理，对于提示词往往无法兼顾
- 开始全面转用推理模型作为基础模型，并且添加组件收纳思考过程，兼顾了美观，极大提升了回答效果

### 在添加新的知识库后由于文件数量过大导致检索速度及其缓慢
- 主要原因在于混合检索模式下，关键词检索算法的重复运行。通过修改算法库函数，提前预处理所有文本块，使实际运行时提升速度8倍左右

### 常规RAG模式在数控机床说明书这一类涉及大量数字参数的场景，检索效果会严重被无关信息影响
- 通过修改知识库的准入流程来优化知识库结构，实现了RAG对关键信息的精准捕捉。优化后的文档处理流程如下：
    - 在将pdf文件ocr为md文件后，先通过正则表达式进行乱码信息清洗。
	@@ -27,16 +31,35 @@
    - 在处理完所有分块后，将处理完的文本块合并为一个清洗完成的md文件输出作为参考文档。为了避免原有的文件处理代码混乱，因此将分块表和问题表导出为自定文件格式:.chunk和.ques
    - 随后将分块表以及问题表共同添加到知识库中，使原文本和标准问题共同被检索，互相弥补。在对数字等文本难以检索的情况下，预设的标准问题会被优先检索，并从数据库中查询对应的文本块并返回。普通场景下，语义丰富的文本块则会被优先检索。所有检索到的文本均会被添加元数据用于联系上下文，并且重复检索的文本也可以借此去重。
    - 为了更进一步避免小样本检索下容易错失一些关键文本，得益于对检索速度的优化，在初步检索时检索的信息条数设置为100条，再通过拥有独立优化的重排模型，根据用户提问对所有检索到的文本块进行打分。从中选择得分最高的8个文本块作为亚龙大模型回答的参考资料。经过实践，检索效果拥有显著的提高，并且通常不会超出亚龙大模型的上下文处理能力。
- 新的处理流程还拥有以下优势
    >在处理参数检索问题时，例如对于"Fanuc系统参数1020的含义"这一类问题的检索，常规情况下检索到的内容如下：
    >较好的情况："12.2.4.2 显示目录树 .. ... **1020**"  这里的1020为页码
    >普遍的情况： "**1020**2 | 2277#5,6,7 2278#0,2.4 | 参数设定不同。","| N0**1020**Q1A1P88A2P89A3P90A4P..; |"
    >
    >检索效果极易受到无关数字影响，尽管它们可能相关，可能无关，可惜都不是我们需要找的文本。
    >如果需要添加一个文本文件作为补丁，那么1020无法提升被检索到的权重，因此需要添加如下文本"Fanuc数控系统参数1020的含义：复制内容..."
    >
    >这理论上是有效的，那么对于相同的问题按照相同的方式添加补丁，为了避免文本分块造成的影响，就需要重新建立一个文件。这就会导致知识库的管理混乱，不易于批量插入。而且我们还需要思考一个问题，既然此处易检索，原处难检索，那么如此处理知识库中就出现了两份完全相同的内容。长此以往就会造成严重的冗余。而事实上，往往这并不有效。
    >
    >对于新的流程，我们可以将"Fanuc系统参数1020的含义"作为一个标准问题直接插入，并指向数据库中的文本块，这样即使是批量的插入也易于管理，而且不出现冗余。并且由于存在更多与提问直接强相关的内容，因此更不容易被无关数字影响。就算事发突然，难检索的原文突然被检索到，也可以通过元数据去重。
    >
    >也许这种方式在实际应用下会出现新的问题，但是也确实解决了目前的一些痛点。
    - 对于常规RAG，如果想改善某一问题的检索结果，只能在文本上添加补丁文本，或是修改原有文本。但是往往收效甚微。
        - 新的处理流程可以直接自行添加一些标准问题作为补丁，直接添加到知识库中。这样的处理方式可以取得立竿见影的效果。

### 使用模型对大样本的文档进行清洗和预设标准问题，时间和报错成本非常高，正常情况下需要超过半周以上的时间。
- 定制了一套并发算法让多个模型同时协作处理一个文档，将整个流程缩短到8小时(4节点)。并发流程如下：
    - 每一个模型单独作为一个节点在程序中注册，由于每个模型部署在不同的机器上，处理能力不同，因此注册时备注其优先级以及处理能力。
    - 在文档处理过程中，分块在申请节点处理时，高优先级的节点会被优先分配。如果超出了处理能力，那么会重新等待分配。
    - 如果在多次等待后如果仍然未被处理，或是在处理时出现错误，处理后效果不符合预期，那么这个块将会被推送到失败队列，由能力最强的一批模型单独处理。如果再次出现错误，那么保留原文本。
    - 处理过程中所有的信息和节点处理结果都会被统计，并且被保存到日志中。即使中途被停止，也可以从最后一个被处理的文件开始继续任务。经过以上流程处理，在实际的运行中失败的任务数量为0。

# 目前系统仍然存在的一些问题

### RAG情境下难以维持多轮对话。目前RAG对话轮数为0，即模型不携带此前的历史对话。
- 目前使用的基础模型为QwQ-32B-AWQ-int4，拥有较强的推理能力和指令执行能力。但是部署后已经占用了显卡的所有显存，很难维持足够长的上下文。目前用于检索的嵌入模型和用于文本打分的重排模型均部署于CPU或者其他机器，一定程度上影响了RAG的应答速度。
- 由于RAG检索会携带大量文本，因此在24G显存的4090上虽然能得到比较满意的处理速度，但是单轮问答就逼近上下文极限。若文本再长就会导致截断。

### 模型参数偏小，对于复杂的指令难以处理
- 目前市面上厂家主推的模型有：通义千问Qwen3 235B,DeepSeekV3 671B。通常情况下70B左右的模型就已经能达到相当完整的能力。
- 由于亚龙大模型的基础模型参数量偏小，在处理复杂指令时很容易出现遗漏和幻觉。而参数量更大的模型则需要更大显存的显卡进行支撑。
	@@ -47,4 +70,5 @@
- 增加多模态的处理能力。
    - 目前文档中的图片内容全部在清洗过程中去除，如果可以通过对图片处理并加以利用，可能能得到更好的回答，更精细的指导。
    - 可以在对话界面调用摄像头，让模型理解视频内容的基础上进行对话，初步对多模态实验指导进行尝试。
- 实现多语言切换，满足不同语种客户需求。
- 为了满足以上需求，可能对网页端使用html进行重构。目前使用streamlit，难以满足高性能要求。